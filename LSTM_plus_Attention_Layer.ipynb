{
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layer "
      ],
      "metadata": {
        "id": "4dLgNvjnqFQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras==2.1.5"
      ],
      "metadata": {
        "trusted": true,
        "id": "ALqaEk9C9vFR",
        "outputId": "f350422c-37c8-4dac-8cce-9d36aaa213a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting keras==2.1.5\n  Downloading Keras-2.1.5-py2.py3-none-any.whl (334 kB)\n\u001b[K     |████████████████████████████████| 334 kB 4.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras==2.1.5) (1.20.1)\nRequirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras==2.1.5) (5.4.1)\nRequirement already satisfied: scipy>=0.14 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras==2.1.5) (1.6.1)\nRequirement already satisfied: six>=1.9.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras==2.1.5) (1.15.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.1.5\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    \"\"\"Attention layer implementation based in the work of Yang et al. \"Hierarchical\n",
        "    Attention Networks for Document Classification\". This implementation also allows\n",
        "    changing the common tanh activation function used on the attention layer, as Chen\n",
        "    et al. \"A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task\"\n",
        "    point that removing this component can be beneficial to the model. Supports\n",
        "    masking.\n",
        "    \n",
        "    The mathematical formulation of the model is as follows:\n",
        "    ```\n",
        "    u = f(W * h + b),\n",
        "    a_i = softmax(u_i^T * u_s),\n",
        "    v_i = \\sigma_i a_i * h_i.\n",
        "    ```\n",
        "    \n",
        "    Where h are the input tensors with shape (batch, n_timesteps, hidden_size), for\n",
        "    instance, all hidden vectors produced by a recurrent layer, such as a LSTM and the\n",
        "    output has shape `(batch, hidden_size)`. This layer also works with inputs with more\n",
        "    than 3 dimensions as well, such as sentences in a document, where each input has\n",
        "    size (batch, n_docs, n_sentences, embedding_size), outputing \n",
        "    (batch, n_docs, embedding_size)`.\n",
        "    \n",
        "    \n",
        "    # Arguments\n",
        "        activation: The activation function f used by the layer (see\n",
        "            [activations](../activations.md)). By default tanh is used, another common\n",
        "            option is \"linear\".\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        initializer: Initializer for the `kernel` and `context` matrices\n",
        "            (see [initializers](../initializers.md)).\n",
        "        return_attention: If True, instead of returning the sequence descriptor, this\n",
        "            layer will return the computed attention coefficients for each of the\n",
        "            sequence timesteps. See Output section for details.\n",
        "        W_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        u_regularizer: Regularizer function applied to the `context` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        b_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        W_constraint: Constraint function applied to the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        u_constraint: Constraint function applied to the `contextl` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        b_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "\n",
        "    # Input shape\n",
        "        nD tensor with shape: `(batch_size, ..., timesteps, input_dim)`.\n",
        "        The most common situation would be a 3D input with shape\n",
        "        `(batch_size, timesteps, input_dim)`.\n",
        "\n",
        "    # Outuput shape\n",
        "        The sequence descriptor with shape `(batch_size, ..., timestamps)`. If\n",
        "        `return_attention` is True, this layer will return the `alpha_i` weights\n",
        "        for each timestep, and consequently its output shape will be different, namely:\n",
        "        `(batch_size, ..., timesteps)`.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "Yrqd5d5cWC-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.layers import Layer\n",
        "from keras import initializers\n",
        "from keras import activations\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self,activation='tanh',initializer='glorot_uniform',return_attention=False,\n",
        "                 W_regularizer=None,u_regularizer=None,b_regularizer=None,W_constraint=None,u_constraint=None,b_constraint=None,bias=True,**kwargs):\n",
        "        self.activation = activations.get(activation)\n",
        "        self.initializer = initializers.get(initializer)\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "        self.bias = bias\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # input_shape: (batch, time, amount_features), the attention size matches the feature dimension\n",
        "        amount_features, attention_size = input_shape[-1],input_shape[-1]\n",
        "\n",
        "        self.W = self.add_weight(shape=(amount_features, attention_size),\n",
        "                                 initializer=self.initializer,regularizer=self.W_regularizer,constraint=self.W_constraint,name='attention_W')\n",
        "        self.b = None\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(attention_size,),initializer='zero',regularizer=self.b_regularizer,constraint=self.b_constraint,name='attention_b')\n",
        "\n",
        "        self.context = self.add_weight(shape=(attention_size,),initializer=self.initializer,regularizer=self.u_regularizer,constraint=self.u_constraint,name='attention_us')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):        \n",
        "        # U = tanh(H*W + b) (eq. 8)        \n",
        "        ui = K.dot(x, self.W)              # (b, t, a)\n",
        "        if self.b is not None:\n",
        "            ui += self.b\n",
        "        ui = self.activation(ui)           # (b, t, a)\n",
        "\n",
        "        # Z = U * us (eq. 9: softmax function: exp(ui^tus)/sum(exp(ui^t,us)),us: sentence level context vector, use it to measure sentence importance)\n",
        "        us = K.expand_dims(self.context)   # (a, 1)\n",
        "        ui_us = K.dot(ui, us)              # (b, t, a) * (a, 1) = (b, t, 1)\n",
        "        ui_us = K.squeeze(ui_us, axis=-1)  # (b, t, 1) -> (b, t)\n",
        "        \n",
        "        # alpha = softmax(Z) (eq. 9)\n",
        "        alpha = self._masked_softmax(ui_us, mask) # (b, t), sum(exp(ui^t,us))\n",
        "        alpha = K.expand_dims(alpha, axis=-1)     # (b, t, 1)\n",
        "        \n",
        "        if self.return_attention:\n",
        "            return alpha\n",
        "        else:\n",
        "            # v = alpha_i * x_i (eq. 10)\n",
        "            return K.sum(x * alpha, axis=1)\n",
        "    \n",
        "    def _masked_softmax(self, logits, mask): #https://www.programcreek.com/python/example/93679/keras.backend.cast\n",
        "        b = K.max(logits, axis=-1, keepdims=True)\n",
        "        logits = logits - b\n",
        "        exped = K.exp(logits)\n",
        "\n",
        "        # ignoring masked inputs\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx()) ## masked timesteps have zero weight\n",
        "            exped *= mask\n",
        "        partition = K.sum(exped, axis=-1, keepdims=True)\n",
        "\n",
        "        # if all timesteps are masked, the partition will be zero. To avoid this issue we use the following trick:\n",
        "        partition = K.maximum(partition, K.epsilon())\n",
        "        return exped / partition\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"The attention mechanism computes a weighted average between all hidden vectors generated by the previous sequential layer, hence the input is expected to be\n",
        "        `(batch_size, seq_len, amount_features)` if `return_attention` is `False`, otherwise the output should be (batch_size, seq_len).\"\"\"\n",
        "        if self.return_attention:\n",
        "            return input_shape[:-1]\n",
        "        else:\n",
        "            return input_shape[:-2] + input_shape[-1:]\n",
        "\n",
        "    def compute_mask(self, x, input_mask=None):\n",
        "        \"\"\"This layer produces a single attended vector from a list of hidden vectors, hence it can't be masked as this means masking a single vector.\"\"\"\n",
        "        return None\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'activation': self.activation,'initializer': self.initializer,'return_attention': self.return_attention,\n",
        "            'W_regularizer': initializers.serialize(self.W_regularizer),'u_regularizer': initializers.serialize(self.u_regularizer),'b_regularizer': initializers.serialize(self.b_regularizer),\n",
        "            'W_constraint': constraints.serialize(self.W_constraint),'u_constraint': constraints.serialize(self.u_constraint),'b_constraint': constraints.serialize(self.b_constraint),\n",
        "            'bias': self.bias}\n",
        "\n",
        "        base_config = super().get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "metadata": {
        "id": "FlXqknQNlBrD",
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "trusted": true,
        "id": "PKYh2hKh9vFm",
        "outputId": "789f2c95-08d2-4d8c-9e2b-156ad40084c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting nltk\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 4.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (1.0.1)\nCollecting regex\n  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n\u001b[K     |████████████████████████████████| 720 kB 16.9 MB/s eta 0:00:01\n\u001b[?25hCollecting tqdm\n  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n\u001b[K     |████████████████████████████████| 75 kB 5.4 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434675 sha256=78a64688a46d1f182edae14cc9d36e6dfaf048c9d3823bd659fb0920987e9172\n  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\nSuccessfully built nltk\nInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.5 regex-2021.4.4 tqdm-4.60.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from IPython.display import display, Markdown\n",
        "from keras import models\n",
        "from keras import optimizers as opt\n",
        "from keras.layers import Input\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGzyEd8NkIDm",
        "outputId": "48a187af-63b4-4d57-aa60-fa61c5c32d1c",
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "vow6BaxFqOlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import data"
      ],
      "metadata": {
        "id": "7yUYxRJEqUTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "import string\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "#rate the transcript data\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Fordham_research/hate_speech/df_train.csv\") #['Tokenized_tweets']\n",
        "val = pd.read_csv(\"/content/drive/MyDrive/Fordham_research/hate_speech/df_val.csv\") #['Tokenized_tweets']\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Fordham_research/hate_speech/df_test.csv\") #['Tokenized_tweets']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xmFXfLYYEh7",
        "outputId": "2f7574f4-b363-4b93-bc3d-6cbdf148ebb9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([train,val,test])\n",
        "data['class'] = data['class'].replace({'Non_Hate':0,'Racist':1,'Offensive':2,'Sexist':3,'religion_offensive':4},inplace=False)"
      ],
      "metadata": {
        "id": "F1FObN0rdeBD"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "wohtb-B9jVHs",
        "outputId": "1bcb5218-80ec-41a3-8100-831fb7449f8d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  class                                              tweet  \\\n",
              "0       17837      1  @RoseGold_C Okurrr like nigga go daaataway lma...   \n",
              "1        3506      1  @KarlousM NIGGA SAID JESUS NEPHEW @mynamekayo ...   \n",
              "2       17507      0  Thinks that Diversity are SOOO much better tha...   \n",
              "3        1874      1  @Pileofblackslag @prince__hubris This faggot d...   \n",
              "4         447      4  Not sure I want religious symbols on my servos...   \n",
              "\n",
              "                                    Tokenized_tweets  \n",
              "0          Okurrr like nigga go daaataway lmao https  \n",
              "1  KarlousM NIGGA SAID JESUS NEPHEW mynamekayo https  \n",
              "2  Thinks Diversity SOOO much better flawless I t...  \n",
              "3  Pileofblackslag This faggot doesnt like tummie...  \n",
              "4  Not sure I want religious symbols servos Not I...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0e32c8c-7e95-4c09-9688-ebaa0535e6d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "      <th>Tokenized_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17837</td>\n",
              "      <td>1</td>\n",
              "      <td>@RoseGold_C Okurrr like nigga go daaataway lma...</td>\n",
              "      <td>Okurrr like nigga go daaataway lmao https</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3506</td>\n",
              "      <td>1</td>\n",
              "      <td>@KarlousM NIGGA SAID JESUS NEPHEW @mynamekayo ...</td>\n",
              "      <td>KarlousM NIGGA SAID JESUS NEPHEW mynamekayo https</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17507</td>\n",
              "      <td>0</td>\n",
              "      <td>Thinks that Diversity are SOOO much better tha...</td>\n",
              "      <td>Thinks Diversity SOOO much better flawless I t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1874</td>\n",
              "      <td>1</td>\n",
              "      <td>@Pileofblackslag @prince__hubris This faggot d...</td>\n",
              "      <td>Pileofblackslag This faggot doesnt like tummie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>447</td>\n",
              "      <td>4</td>\n",
              "      <td>Not sure I want religious symbols on my servos...</td>\n",
              "      <td>Not sure I want religious symbols servos Not I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0e32c8c-7e95-4c09-9688-ebaa0535e6d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0e32c8c-7e95-4c09-9688-ebaa0535e6d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0e32c8c-7e95-4c09-9688-ebaa0535e6d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#data = data.loc[(data['class']=='Non_Hate') | (data['class']=='Racist')]\n",
        "#data.loc[data['class']=='Non_Hate','class']=0\n",
        "#data.loc[data['class']=='Racist','class']=1\n",
        "# data.loc[data['class']=='Offensive','class']=2\n",
        "#data.loc[data['class']=='Sexist','class']=1\n",
        "# data.loc[data['class']=='religion_offensive','class']=4\n",
        "\n",
        "#Stanford_dataset.loc[Stanford_dataset['class'] == 0,'class'] = 1\n",
        "samples = list(data['tweet'].values)\n",
        "labels = list(data['class'].values)\n",
        "class_names = ['Non_Hate','Racist','Offensive','Sexist','religion_offensive']\n"
      ],
      "metadata": {
        "id": "ERp0F6gtCg34",
        "trusted": true
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing"
      ],
      "metadata": {
        "id": "vQdAy8LQqZv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(samples).batch(128)\n",
        "vectorizer.adapt(text_ds)\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "bPqjT1t7Coxr"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  transformed_text = []\n",
        "  stem = nltk.stem.SnowballStemmer('english')\n",
        "  text = text.lower()\n",
        "  for token in nltk.word_tokenize(text):\n",
        "    # if punctuation, pass\n",
        "    if token in string.punctuation: continue\n",
        "    if 'http' in token: continue\n",
        "    if '.com' in token: continue\n",
        "    if '@' in token: continue\n",
        "    if 'rt' == token: continue\n",
        "    # if unknown words, return UNK\n",
        "    if token not in word_index.keys():transformed_text.append(word_index['[UNK]'])\n",
        "    # if known words\n",
        "    if token in word_index.keys():transformed_text.append(word_index[token])\n",
        "  return transformed_text"
      ],
      "metadata": {
        "id": "dh1sR6eCIy48"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer text into word indexes\n",
        "samples_index = [tokenize(a) for a in samples]\n"
      ],
      "metadata": {
        "id": "0p9_vxeRKVZq"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_backup = samples\n",
        "test = np.array(samples)"
      ],
      "metadata": {
        "id": "YDO9KvVFMssm"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "trusted": true,
        "id": "djYwi6v69vF7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tf.keras.preprocessing.text import Tokenizer\n",
        "# Tokenize text data\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(samples)\n",
        "samples = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "#tokenizer.fit_on_texts(x_test)\n",
        "#x_test = tokenizer.texts_to_sequences(x_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "rD1aWd7m9vF7"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]\n",
        "\n",
        "train_samples = np.array(train_samples)\n",
        "val_samples = np.array(val_samples)\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "#class_weights = class_weight.compute_class_weight('balanced',np.unique(train_labels),train_labels)"
      ],
      "metadata": {
        "id": "veM43JHMCmv0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0a5551-871c-4520-df10-b0646bdd5717"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average length for a given tweet, get this for setting a proper max_length for each tweet\n",
        "sum([len(item) for item in train_samples])/len(train_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HpedJwkNoJf",
        "outputId": "cd179e21-0e0e-4e36-995f-101b59d38c05",
        "trusted": true
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.259658393492417"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad text to sequence\n",
        "VOCAB_SIZE = 200000\n",
        "MAX_LEN = 128\n",
        "HIDDEN_SIZE = 100\n",
        "DROPOUT = 0.5\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "x_train = pad_sequences(train_samples, MAX_LEN, padding='post', truncating='post') # 'post' is adding padding at the end\n",
        "x_val = pad_sequences(val_samples, MAX_LEN, padding='post', truncating='post') "
      ],
      "metadata": {
        "id": "323ZG_uRNQiu",
        "trusted": true
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "y_RhvVNDqg-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    sentence_in = Input((None, ),name='sentence-in') #(shape d:input will be batches of d-dimensional vectors,batch_size,name)\n",
        "    #(input_dim: Size of the vocabulary, output_dim: Dimension of the dense embedding, \n",
        "    #mask_zero: whether the input value 0 is a special \"padding\" value that should be masked out, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1))\n",
        "    embedded = Embedding(VOCAB_SIZE, HIDDEN_SIZE, mask_zero=True, name='embedding')(sentence_in)\n",
        "    #(Positive integer, dimensionality of the output space, Whether to return the last output in the output sequence, Fraction of the units to drop for the linear transformation of the inputs)\n",
        "    vectors  = LSTM(HIDDEN_SIZE*2,return_sequences=True, dropout=DROPOUT,recurrent_dropout=DROPOUT,name='ff-lstm')(embedded)\n",
        "    vectors2  = LSTM(HIDDEN_SIZE,return_sequences=True, dropout=DROPOUT,recurrent_dropout=DROPOUT,name='ff2-lstm')(vectors) \n",
        "    vectors3  = LSTM(HIDDEN_SIZE,return_sequences=True, dropout=DROPOUT,recurrent_dropout=DROPOUT,name='ff3-lstm')(vectors2)\n",
        "    sentence = AttentionLayer(name='attention')(vectors3) \n",
        "    output   = Dense(len(class_names),activation='sigmoid',name='output')(sentence)\n",
        "    model = models.Model(inputs=[sentence_in], outputs=[output])\n",
        "    return model"
      ],
      "metadata": {
        "id": "JkP-wC__OIbO",
        "trusted": true
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ySpQYgjns3",
        "outputId": "dd8b4965-f356-41e6-cb00-6f2372b83539"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 0, ..., 4, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (b, t, d)\n",
        "model = build_model()\n",
        "model.summary()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "model.fit(x_train, train_labels, batch_size=32, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9FiEzLDOKlm",
        "outputId": "98389efd-4bc6-4571-e4f7-899f82237732",
        "trusted": true
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sentence-in (InputLayer)    [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         20000000  \n",
            "                                                                 \n",
            " ff-lstm (LSTM)              (None, None, 200)         240800    \n",
            "                                                                 \n",
            " ff2-lstm (LSTM)             (None, None, 100)         120400    \n",
            "                                                                 \n",
            " ff3-lstm (LSTM)             (None, None, 100)         80400     \n",
            "                                                                 \n",
            " attention (AttentionLayer)  (None, 100)               10200     \n",
            "                                                                 \n",
            " output (Dense)              (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,452,305\n",
            "Trainable params: 20,452,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1202/1202 [==============================] - 1910s 2s/step - loss: 0.3546 - acc: 0.8583 - val_loss: 0.2740 - val_acc: 0.8955\n",
            "Epoch 2/10\n",
            "1202/1202 [==============================] - 1873s 2s/step - loss: 0.2685 - acc: 0.8958 - val_loss: 0.2655 - val_acc: 0.8978\n",
            "Epoch 3/10\n",
            "1202/1202 [==============================] - 1914s 2s/step - loss: 0.2539 - acc: 0.9006 - val_loss: 0.2751 - val_acc: 0.8964\n",
            "Epoch 4/10\n",
            "1202/1202 [==============================] - 1893s 2s/step - loss: 0.1550 - acc: 0.9405 - val_loss: 0.2828 - val_acc: 0.8806\n",
            "Epoch 5/10\n",
            "1202/1202 [==============================] - 2048s 2s/step - loss: 0.0572 - acc: 0.9812 - val_loss: 0.4524 - val_acc: 0.8591\n",
            "Epoch 6/10\n",
            "1202/1202 [==============================] - 1988s 2s/step - loss: 0.0242 - acc: 0.9932 - val_loss: 0.5762 - val_acc: 0.8291\n",
            "Epoch 7/10\n",
            "1202/1202 [==============================] - 1981s 2s/step - loss: 0.0125 - acc: 0.9972 - val_loss: 0.7747 - val_acc: 0.8337\n",
            "Epoch 8/10\n",
            "1202/1202 [==============================] - 1983s 2s/step - loss: 0.0104 - acc: 0.9978 - val_loss: 0.7347 - val_acc: 0.8290\n",
            "Epoch 9/10\n",
            "1202/1202 [==============================] - 1934s 2s/step - loss: 0.0080 - acc: 0.9985 - val_loss: 1.1641 - val_acc: 0.8272\n",
            "Epoch 10/10\n",
            "1202/1202 [==============================] - 1944s 2s/step - loss: 0.0073 - acc: 0.9986 - val_loss: 0.8668 - val_acc: 0.8426\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2daae16790>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "cGbh9jLgqmNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score,recall_score, classification_report"
      ],
      "metadata": {
        "id": "lLn4sLGWVJ8p"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['class'] = test['class'].replace({'Non_Hate':0,'Racist':1,'Offensive':2,'Sexist':3,'religion_offensive':4},inplace=False)\n",
        "test_text = test['Tokenized_tweets']\n",
        "test_labels = test['class']\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(test_text)\n",
        "test_token = tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "test_samples = pad_sequences(x_val, MAX_LEN, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "9Z7_tDe-Y6C1"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = model.predict(test_samples)"
      ],
      "metadata": {
        "id": "NTGHzH3kVL8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6f315d-e450-4790-cf76-8dda975dd79a"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "376/376 [==============================] - 79s 209ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict = predict.round().astype(int)"
      ],
      "metadata": {
        "id": "JVmPbfmqVM3a"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for i in predict:\n",
        "  if (i == [1,0,0,0,0]).all():\n",
        "    pred.append(0)\n",
        "  elif (i == [0,1,0,0,0]).all():\n",
        "    pred.append(1)\n",
        "  elif (i == [0,0,1,0,0]).all():\n",
        "    pred.append(2)\n",
        "  elif (i == [0,0,0,1,0]).all():\n",
        "    pred.append(3)\n",
        "  else:\n",
        "    pred.append(4)"
      ],
      "metadata": {
        "id": "GmOh6AHAVNz1"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['Non_Hate','Racist','Offensive','Sexist','religion_offensive']\n",
        "\n",
        "print(classification_report(test_labels, pred))"
      ],
      "metadata": {
        "id": "OfJYsivyVPOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Attention Weights"
      ],
      "metadata": {
        "id": "4BrV7XlbqrRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_viz_model(trained_model):\n",
        "    \"\"\"Architecture: input -> embedding -> lstm -> attention -> sigmoid\"\"\"\n",
        "    sentence_in = Input((None, ),name='sentence-in')\n",
        "    embedded = Embedding(VOCAB_SIZE,HIDDEN_SIZE,mask_zero=True,weights=trained_model.layers[1].get_weights(),name='embedding')(sentence_in)\n",
        "    vectors  = LSTM(HIDDEN_SIZE,return_sequences=True,dropout=DROPOUT,recurrent_dropout=DROPOUT,weights=trained_model.layers[2].get_weights(),name='ff-lstm')(embedded)\n",
        "    alphas  = AttentionLayer(weights=trained_model.layers[3].get_weights(),return_attention=True,name='attention')(vectors)\n",
        "    model = models.Model(inputs=[sentence_in], outputs=[alphas])\n",
        "    return model"
      ],
      "metadata": {
        "id": "THYV7BAyhasL"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz_model = build_viz_model(model)"
      ],
      "metadata": {
        "id": "a2fLaIVMhbQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weights(df):\n",
        "  word = df['word']\n",
        "  words = [i for i in range(len(word))]\n",
        "  attention_weight = df['attention_weight']\n",
        "  plt.plot(words, attention_weight)\n",
        "  plt.xticks(words, word)\n",
        "  plt.legend(labels = ['attention weight'])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "g-N3PSzw3Uag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention(text):\n",
        "  encoded_txt = np.array([tokenize(text)])\n",
        "  weights = viz_model(encoded_txt)[0]\n",
        "  weights = [float(w) for w in weights]\n",
        "  words = [voc[w] for w in tokenize(text)]\n",
        "  df = pd.DataFrame([words,weights]).T\n",
        "  df.columns = ['word','attention_weight']\n",
        "  return df"
      ],
      "metadata": {
        "id": "2pXJfC9eeuZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz_model.save('/content/drive/MyDrive/Project/Tweet Datasets/Model/racist_viz_model')\n",
        "model.save('/content/drive/MyDrive/Project/Tweet Datasets/Model/racist_trained_model')"
      ],
      "metadata": {
        "id": "Bwp4kEJh1M9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vaderSentiment"
      ],
      "metadata": {
        "id": "nC91gDr4VfkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        " \n",
        "# function to print sentiments\n",
        "# of the sentence.\n",
        "def sentiment_scores(sentence):\n",
        " \n",
        "    # Create a SentimentIntensityAnalyzer object.\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        " \n",
        "    # polarity_scores method of SentimentIntensityAnalyzer\n",
        "    # object gives a sentiment dictionary.\n",
        "    # which contains pos, neg, neu, and compound scores.\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "     \n",
        "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
        "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
        "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
        " \n",
        "    print(\"Sentence Overall Rated As\", end = \" \")\n",
        " "
      ],
      "metadata": {
        "id": "brq8YSK9Vblr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YT7f7UWWVbrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}